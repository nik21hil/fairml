# fairml

**Bias Detection and Mitigation Toolkit for Responsible AI**

---

## ğŸ“ **Overview**

`fairml` is a Python package designed to help data scientists and machine learning engineers **detect, visualize, and mitigate bias** in datasets and models seamlessly.

With intuitive APIs, rich visualizations, and practical mitigation techniques, `fairml` makes fairness-aware AI development accessible to every practitioner.

---

## âš¡ **Key Features**

âœ… Bias detection metrics  
âœ… Bias visualization dashboards  
âœ… Pre-processing and post-processing mitigation methods  
âœ… scikit-learn compatible pipelines  
âœ… Lightweight, modular, and extensible design

---

## ğŸ’» **Installation**

```
pip install fairml
```

*(Currently under development; install via GitHub for now)*

```
pip install git+https://github.com/nik21hil/fairml.git
```

---

## ğŸš€ **Quickstart**

```python
from fairml import detection

# Example: Calculate Statistical Parity Difference
spd = detection.statistical_parity_difference(y_pred, sensitive_features, privileged_group='A', unprivileged_group='B')
print("Statistical Parity Difference:", spd)
```

More detailed examples and notebooks are available in the `examples/` folder.

---

## ğŸ“Š **Modules**

- `detection.py`: Bias detection metrics  
- `visualization.py`: Fairness visualizations and plots  
- `mitigation.py`: Bias mitigation algorithms  
- `utils.py`: Helper functions

---

## ğŸ”§ **Fairness Mitigation Utilities**

The `mitigation` module includes several techniques to reduce class imbalance and improve fairness.

### âœ… Supported Methods

| Method                       | Type           | Description                                                                 |
|-----------------------------|----------------|-----------------------------------------------------------------------------|
| `apply_smote`               | Over-sampling  | Generates synthetic minority samples using k-nearest neighbors             |
| `apply_adasyn`              | Over-sampling  | Focuses more on hard-to-learn minority samples                             |
| `combined_resample`         | Combo          | Combines `SMOTE` with `Tomek Links` for noise reduction                    |
| `apply_cluster_centroids`   | Under-sampling | Replaces majority class samples with cluster centroids using KMeans       |

---

## ğŸ§ª **Usage Examples**

```python
from fairml.mitigation import (
    apply_smote,
    apply_adasyn,
    combined_resample,
    apply_cluster_centroids
)

# Sample imbalanced data
X = pd.DataFrame({'f1': range(30)})
y = np.array([0] * 25 + [1] * 5)

# SMOTE
X_sm, y_sm = apply_smote(X, y)

# ADASYN
X_ad, y_ad = apply_adasyn(X, y)

# SMOTE + Tomek Links
X_comb, y_comb = combined_resample(X, y, strategy='smote_tomek')

# Cluster Centroids
X_cc, y_cc = apply_cluster_centroids(X, y)
```

---

### ğŸ›  **When to Use What?**

| Scenario                                | Recommended Method     |
|----------------------------------------|------------------------|
| Moderate imbalance                     | `apply_smote`          |
| Imbalance + noisy boundaries           | `combined_resample`    |
| Focus on harder minority examples      | `apply_adasyn`         |
| Overwhelming majority class size       | `apply_cluster_centroids` |

---

## ğŸ“ **Project Structure**

```
fairml/
â”‚
â”œâ”€â”€ fairml/
â”‚   â”œâ”€â”€ detection.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â”œâ”€â”€ mitigation.py
â”‚   â”œâ”€â”€ utils.py
â”‚
â”œâ”€â”€ examples/
â”œâ”€â”€ tests/
â”œâ”€â”€ setup.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
```

---

## âœ… **Roadmap**

- [x] Phase 0: Project setup  
- [x] Phase 1: Core bias detection metrics  
- [x] Phase 2: Visualization module  
- [x] Phase 3: Pre-processing mitigation techniques  
- [x] Phase 4: Post-processing mitigation techniques  
- [ ] Phase 5: End-to-end example notebooks  
- [ ] Phase 6: PyPI release

---

## ğŸ¤ **Contributing**

Contributions are welcome. Please open an issue or pull request to discuss improvements, features, or bug fixes.

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## âœ¨ **Acknowledgements**

- IBM AI Fairness 360  
- Microsoft Fairlearn  
- The broader AI fairness and ethics research community

---

## ğŸŒ **Author**

**Nikhil Singh**  
[GitHub](https://github.com/nik21hil) | [LinkedIn](https://www.linkedin.com/in/nikhil-singh21/)
